---
title: "cdcdata-exercise.qmd"
author: Murtaza Yaqubi
editor: visual
---

First we start by installing and loading necessary libraries for our analysis as per usual.

```{r}

library(here)
library(tidyverse)
library(dplyr)
library(readr)
library(naniar)
library(ggplot2)
library(scales)
library(forcats)
```

### Introduction:

This dataset contains information on heart disease mortality among U.S. adults aged 35 and older, stratified by state/territory and county from 2019 to 2021. The data was obtained from the CDC website (https://data.cdc.gov/Heart-Disease-Stroke-Prevention/Heart-Disease-Mortality-Data-Among-US-Adults-35-by/55yu-xksw/data_preview).

Key variables in the dataset include Year, Location (State, County), Geographic Level, Class, Data Source, Race, Gender, and others. For this analysis, the focus will be on the variables: Year, Location (State), Gender, and Race.

### Importing data:

We will import our dataset and take an initial look at the data.

```{r}
getwd()
cdc_heart <- read_csv("Data/Heart disease mortality among US adults.csv")

head(cdc_heart)  # get a preview of the data set

dim(cdc_heart)   # get the dimensions of the data set

summary(cdc_heart)   # get a summary of the data set

glimpse(cdc_heart)    # get a glimpse of the data set
```

### Processing the data:

First, we'll clean the data by renaming variables to make them more understandable. Then, we'll remove any `NA`values from the dataset. Next, we'll filter the data by Gender, State, Race, and GeographicLevel to extract relevant observations. After that, we'll select the variables of interest:Year, State, GeographicLevel, Gender, Race and Case_numbers. We'll arrange the data by State and, finally, convert Gender and Race to factors.

```{r}
df_cleaned <- cdc_heart %>% 
  rename(State = LocationAbbr,          # renaming of variables
         Gender = Stratification1,
         Race = Stratification2,
         Mortality = Data_Value) %>% 
  drop_na(Mortality) %>%         # removing NA's 
  filter(Gender != "Overall",     # filtering for Gender to not include overall
         State != "DC",    # filtering for State to not include DC because it is no a state.
         GeographicLevel == "County",   # filter for GeographicLevels to only include county
         Race != "Overall")  %>%   # filtering Race to exclude overall from the observations.  
  select(Year, State, GeographicLevel, Gender, Race, Mortality) %>% # selecting varibales of interest
  arrange(State) %>%   # arranging the order of the column "State"
  mutate(Gender = as.factor(Gender), Race = as.factor(Race))   # converting Gender and factor from characters to factors


view(df_cleaned)   # get a preview of the data set. 
  
gg_miss_var(df_cleaned)   # making sure there aren't any NA's  

summary(df_cleaned)   # get a summary of the data set
```

### Plotting the data:

```{r}

df_cleaned %>%     # generate a plot to get the counts for "State"
  ggplot(aes(State)) +  
  geom_bar(fill = "tomato") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

df_cleaned %>%    # generate a plot to get the counts for "Gender"
  ggplot(aes(Gender, fill = Gender)) +
  geom_bar(width = 0.5, alpha = 0.7) +
  scale_fill_manual(values = c("Male" = "dodgerblue", "Female" = "deeppink")) +
  theme_bw()


# Recode the elements in the race variable so that it can fit in the plot and also add custom colors to the bars. 
df_cleaned %>%     
  mutate(Race = fct_recode(Race, 
                           "Multiracial" = "More than one race",
                           "AI/AN" = "American Indian or Alaska Native", 
                           "NH/PI" = "Native Hawaiian or Other Pacific Islander")) %>% 
  ggplot(aes(Race, fill = Race)) +   # plot to get the counts of "Race"   
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_fill_manual(values = c("White" = "firebrick", 
                               "Black" = "thistle", 
                               "Asian" = "powderblue", 
                               "Hispanic" = "darkorchid", 
                               "AI/AN" = "turquoise", 
                               "NH/PI" = "goldenrod", 
                               "Multiracial" = "chartreuse"))

# Plot Gender and Race by mortality.
df_cleaned %>% 
  ggplot(aes(Gender, Mortality, fill = factor(Race))) +
  geom_col() + 
  coord_flip() +   # flip the chart to the side
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  facet_wrap(~Race) +   # create individual charts by race
  ggtitle("Men vs Women mortality rates") +  # title of the chart
  xlab("Mortality per 100,000") +   # label for x axis
  ylab("Race")   # label for y axis


# Create a table that provides the number and percent of mortality pe state. 
state_totals <- df_cleaned %>%
  group_by(State) %>%       # Group by State
  summarise(mortality_cases = sum(Mortality)) %>%  # Sum the cases
  mutate(Percent_mortality = percent_format(accuracy = 0.001)(mortality_cases / sum(mortality_cases))) %>%   # this chunck of code adds percent sign within 3 decimal points.
  arrange(desc(mortality_cases))  # Sort from highest to lowest

print(state_totals)  # View the results

# Ten states with the highest mortality rates.  
top_10 <- state_totals %>% 
  slice_max(order_by = mortality_cases, n = 10) %>% 
  select(everything())

top_10


# Ten states with the lowest mortality rates. 
bottom_10 <- state_totals %>% 
  slice_min(order_by = mortality_cases, n = 10) %>% 
  select(everything()) %>% 
  arrange(-mortality_cases)

bottom_10


# Check the distribution.
df_cleaned %>% 
ggplot(aes(sample = Mortality)) + 
  stat_qq() +       # use Q-Q plot          
  stat_qq_line(color = "tomato") +
  labs(title = "Q-Q Plot for Normality Check") +
  theme_bw()

df_cleaned %>% 
ggplot(aes(x = Mortality)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "midnightblue", color = "black") +  
  geom_density(color = "red", linewidth = 1) +  
  labs(title = "Histogram with Density Curve", x = "Mortality", y = "Density") +
  theme_minimal()

# Check the mean and Standard Deviation. 
mean_mortality <- mean(df_cleaned$Mortality)
mean_mortality

Sd_mortality <- sd(df_cleaned$Mortality)
Sd_mortality

```

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

## **Part 2**

#### This part is contributed to by Shaun van den Hurk

I have used ChatGPT to generate the code to reproduce synthetic data to match the raw data and the data analysis that was performed.

For my prompt I copied the original code and the I copied the results and figures from the original data. I asked it to provide code to generate synthetic data to try to reproduce the original data. I had to provide multiple prompts with the original data and had to correct how graphs were displayed.


```{r}
### Load necessary libraries
library(tidyverse)


### Set seed for reproducibility
set.seed(1234)

### Generate synthetic dataset
num_states <- 50
synthetic_states <- c("AK", "AL", "AR", "AZ", "CA", "CO", "CT", "DE", "FL", "GA", "HI", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "MS", "MT", "NC", "ND", "NE", "NH", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")

### Simulate mortality cases using a normal distribution
synthetic_genders <- sample(c("Male", "Female"), num_states, replace = TRUE)
synthetic_races <- sample(c("White", "Black", "Asian", "Hispanic", "AI/AN", "NH/PI", "Multiracial"), num_states, replace = TRUE)
synthetic_mortality_cases <- round(rnorm(num_states, mean = 100000, sd = 80000))
synthetic_mortality_cases <- pmax(synthetic_mortality_cases, 5000)  # Ensure all values are positive

### Compute synthetic percent mortality
synthetic_percent_mortality <- synthetic_mortality_cases / sum(synthetic_mortality_cases) * 100

### Create synthetic dataframe
synthetic_mortality_data <- tibble(
  Synthetic_State = synthetic_states,
  Synthetic_Gender = synthetic_genders,
  Synthetic_Race = synthetic_races,
  Synthetic_Mortality_Cases = synthetic_mortality_cases
)
```

```{r}
### Display the first few rows
print(head(synthetic_mortality_data))

### Visualize distributions
# Mortality cases by state (alphabetically sorted)
synthetic_mortality_data %>%
  ggplot(aes(x = reorder(Synthetic_State, Synthetic_State), y = Synthetic_Mortality_Cases)) +
  geom_bar(stat = "identity", fill = "tomato") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(title = "Synthetic Mortality Cases by State (Alphabetical Order)", x = "State", y = "Mortality Cases")

# Histogram for mortality cases
synthetic_mortality_data %>%
  ggplot(aes(x = Synthetic_Mortality_Cases)) +
  geom_histogram(bins = 20, fill = "midnightblue", color = "black") +
  geom_density(color = "red", linewidth = 1) +
  labs(title = "Histogram of Synthetic Mortality Cases", x = "Mortality Cases", y = "Density")

# Gender distribution
synthetic_mortality_data %>%
  ggplot(aes(Synthetic_Gender, fill = Synthetic_Gender)) +
  geom_bar(width = 0.5, alpha = 0.7) +
  scale_fill_manual(values = c("Male" = "dodgerblue", "Female" = "deeppink")) +
  theme_bw() +
  labs(title = "Synthetic Gender Distribution", x = "Gender", y = "Count")

# Race distribution
synthetic_mortality_data %>%
  ggplot(aes(Synthetic_Race, fill = Synthetic_Race)) +
  geom_bar() +
  geom_text(stat='count', aes(label=..count..), angle=90, vjust=0.5, hjust=1.1, size=3) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_fill_manual(values = c("White" = "firebrick", "Black" = "thistle", "Asian" = "powderblue", "Hispanic" = "darkorchid", "AI/AN" = "turquoise", "NH/PI" = "goldenrod", "Multiracial" = "chartreuse")) +
  labs(title = "Synthetic Race Distribution", x = "Race", y = "Count")

# Mortality by gender and race
synthetic_mortality_data %>%
  ggplot(aes(Synthetic_Gender, Synthetic_Mortality_Cases, fill = factor(Synthetic_Race))) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  coord_flip() +
  facet_wrap(~Synthetic_Race) +
  labs(title = "Men vs Women Mortality Rates (Synthetic Data)", x = "Mortality per 100,000", y = "Race")
```

I am going to display how close the original and synthetic data is with some summary statistics/tables below

```{r}
### Compare summary statistics with original dataset
original_summary <- tibble(
  Mean_Mortality_Original = 135456.5,
  SD_Mortality_Original = 120374.2,
  Mean_Mortality_Synthetic = mean(synthetic_mortality_cases),
  SD_Mortality_Synthetic = sd(synthetic_mortality_cases)
)
print(original_summary)

### Top 10 States by Mortality Cases
original_top_10 <- tibble(
  Original_State = c("TX", "GA", "OK", "VA", "CA", "NC", "TN", "MS", "OH", "KY"),
  Synthetic_State = c("TX", "GA", "OK", "VA", "CA", "NC", "TN", "MS", "OH", "KY"),
  Synthetic_Mortality_Cases = c(505060.8, 308311.2, 260597.1, 215457.8, 202419.4, 185048.8, 182244.2, 181185.9, 173150.1, 171416.4),
  Synthetic_Percent_Mortality = c(9.566, 5.839, 4.936, 4.081, 3.834, 3.505, 3.452, 3.432, 3.280, 3.247)
)
print(original_top_10)

# Compare original and synthetic top 10 states
synthetic_top_10 <- synthetic_mortality_data %>%
  arrange(desc(Synthetic_Mortality_Cases)) %>%
  slice(1:10)
print(synthetic_top_10)

```

We can see that overall the data is similar to the original data but there are distinct differences in the "shape" of the data, such as the number of males and females and the breakdown of the different race groups within the data which can be seen in the graphs. Adsitionally, the mean and standard deviation for factors such as mortality were somewhat distinct but similar. We could specify that we wanted the data/result to be more closely related to the original mean and standard deviation if desired, depending on what we intend to do with the synthetic data.

I believe that the synthetic data generation focuses on the overall values and not specifics and so it mixes the data up in a way that the overall pattern is similar, although individual variables are different.

If it is important to keep the distribution of the data more closely aligned to the original data within each variable we could specify a range for each variable while generating our synthetic data.This would be important if we want to evaluate certain patterns between variables. But care must be taken to make sure that the synthetic data is not a direct repeat of the original data (especially if there is the risk of identifying information).

If I were to repeat this process I would spend more time on providing a more comprehensive prompt for the AI model to help generate my code. I would have summarised all of the results more clearly and provided the summary and indicated that it was important to produce data that resembles it more closely. I think that breaking things down into key components might have helped for the generation of the synthetic data to be of a greater quality and more closely related to the original data.
However, I think that it still did a good job of producing synthetic data similar to the original set, although with marked differences which could be addressed.

