---
title: "fitting-exercise.qmd"
author: "Murtaza Yaqubi"
editor: visual
---

# Install and load libraries.

```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(readr)
library(here)
library(GGally)
library(corrplot)
library(pROC)
library(tidymodels)
```

# Import data.

```{r}

# Load the data
trial <- read_csv(here::here("fitting-exercise/data/Mavoglurant_A2121_nmpk.csv")) 

# View the dimensions of the data set to insure proper import
dim(trial)

# View summary of the data
summary(trial)

# View column names
colnames(trial)

# Convert DOSE to factor
trial$DOSE <- as.factor(trial$DOSE)
```

# Plot the data.

```{r}

# Plot time against DV
trial %>% 
  ggplot(aes(TIME, DV, group = ID, color = factor(DOSE))) +
  geom_line(alpha = 0.7) +
  labs(title = "Time vs DV by dose",
       x = "Time",
       y = "DV") +
  theme_minimal() +
  theme(legend.position = "right")


# Plot Time agianst DV by Dose
ggplot(trial, aes(x = TIME, y = DV, group = ID, color = factor(DOSE))) +
  geom_line(alpha = 0.7) +
  facet_wrap(~ DOSE, labeller = labeller(dose = function(x) paste("Dose:", x))) +
  scale_color_manual(values = c("50" = "steelblue", "37.5" = "firebrick", "25" = "purple")) +
  labs(title = "Time vs DV by dose",
       x = "Time",
       y = "DV") +
  theme_minimal()

```

Brief description: The two plots represent the relationship between "Time" and "DV" (dependent variable), separated by different doses.

In the first plot, titled "Time vs DV by dose," we see overlapping lines for each dose level (25, 37.5, and 50). The lines show a rapid decline in "DV" as "Time" increases, with the higher dose (50) resulting in a sharper decrease compared to the lower doses (25 and 37.5). The plot helps visualize how different doses affect the "DV" over time, with the higher doses showing a more significant decrease in the dependent variable.

The second plot, titled "Time vs DV by dose," shows a faceted layout where each dose (25, 37.5, and 50) has its own panel. Each panel displays the same trend: "DV" decreases rapidly at the start of the time period and then levels off, with the highest dose (50) exhibiting the most substantial initial drop. The separation of doses into individual panels makes it easier to compare the effects of each dose on "DV" over time.

# Cleaning data.

```{r}
# Filterind OCC to only include 1
occ_trial <- trial %>% 
  filter(OCC == 1) %>% 
  select(everything()) 
  
# View summary of the data
summary(occ_trial)

# Step 1: Exclude observations where TIME == 0 and compute the sum of DV for each individual (ID)
trial_without_time0 <- occ_trial %>%
  filter(TIME != 0) %>%
  group_by(ID) %>%
  summarize(Y = sum(DV, na.rm = TRUE)) %>%
  ungroup()

# Step 2: Create a data frame with only observations where TIME == 0
trial_time0 <- occ_trial %>%
  filter(TIME == 0)

# Step 3: Join the two data frames
final_df <- trial_time0 %>%
  left_join(trial_without_time0, by = "ID")

# View the resulting data frame
print(final_df)


#Convert RACE and SEX to factors and keep only the specified variables
df_cleaned <- final_df %>%
  mutate(
    RACE = as.factor(RACE),
    SEX = as.factor(SEX)
  ) %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT)

# View the resulting data frame
print(df_cleaned)

write_csv(df_cleaned, "../ml-models-exercise/data/df_cleaned.csv")
```

# Exploratory Data Analysis.

```{r}
# Generating summary table
df_summary <- df_cleaned %>%
  summarise(
    mean_Y = mean(Y, na.rm = TRUE),
    sd_Y = sd(Y, na.rm = TRUE),
    mean_AGE = mean(AGE, na.rm = TRUE),
    sd_AGE = sd(AGE, na.rm = TRUE),
    mean_WT = mean(WT, na.rm = TRUE),
    sd_WT = sd(WT, na.rm = TRUE),
    mean_HT = mean(HT, na.rm = TRUE),
    sd_HT = sd(HT, na.rm = TRUE),
    count_dose_25 = sum(DOSE == levels(DOSE)[1], na.rm = TRUE),
    count_dose_37.5 = sum(DOSE == levels(DOSE)[2], na.rm = TRUE),
    count_dose_50 = sum(DOSE == levels(DOSE)[3], na.rm = TRUE)
  )

# View the resulting summary table
print(df_summary)
```

The summary table shows the means and standard deviations of Y, AGE, WT and HT. The table also shows the counts of different levels of dose.

# Plotting the variables.

```{r}
# Scatterplot between Y and DOSE
df_cleaned %>%  
ggplot(aes(DOSE, Y)) +
  geom_point(size = 3, alpha = 0.7, color = "firebrick") +
  labs(title = "Y vs DOSE", x = "DOSE", y = "Y") +
  theme_bw()

# Scatterplot between Y and AGE
df_cleaned %>% 
  ggplot(aes(AGE, Y)) +
  geom_point(size = 3, alpha = 0.7, color = "red") +
  labs(title = "Y vs AGE", x = "AGE", y = "Y") +
  theme_bw()

# Boxplot of Y by SEX
ggplot(df_cleaned, aes(x = SEX, y = Y, fill = SEX)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16) +
  labs(title = "Y by SEX", x = "SEX", y = "Y") +
  theme_minimal() +
  scale_fill_manual(values = c("1" = "skyblue", "2" = "lightpink"))

# Boxplot of Y by RACE
ggplot(df_cleaned, aes(x = RACE, y = Y, fill = RACE)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16) +
  labs(title = "Y by RACE", x = "RACE", y = "Y") +
  theme_minimal() +
  scale_fill_manual(values = c(
    "1"    = "skyblue", 
    "2"    = "lightcoral", 
    "7"    = "lightgreen", 
    "88"    = "plum"
  ))
```

Brief description of the plots :

The purpose of generating these plots is to visually explore the relationships between the outcome variable "Y" and various predictors(SEX, RACE, AGE and DOSE), both continuous and categorical. These plots help in understanding how these predictors might influence or explain the variation in "Y" and give insight into potential trends, patterns, and differences across groups.

1- Y vs DOSE: This scatterplot shows the relationship between "Y" and "DOSE." The data points are grouped around three distinct DOSE values (25, 37.5, and 50), and "Y" has a broad range at each DOSE level. There’s noticeable variation in "Y" at each dose, with dose value of 50 associated with highest "Y" values and dose value of 25 associated with lowest "Y" values.

2- Y vs AGE: This scatterplot displays how "Y" changes with "AGE." The data is spread across different age values, and no clear linear relationship is observed. The spread of "Y" suggests that age has some effect, but the relationship appears complex, with substantial variation in "Y" at each age.

3- Y by SEX: This boxplot compares "Y" between the two SEX categories. The plot shows that "SEX" category 1 has a higher median and a wider range of "Y" values compared to category 2. There are some outliers in both categories, indicating extreme values in "Y" within each group.

4- Y by RACE: This boxplot compares "Y" across different "RACE" categories. There’s variability in "Y" across the races, with race group "2" having a higher median and wider spread of "Y" values. This suggests that "RACE" may influence "Y," with differences in the central tendency and variability across groups.

# Distribution of varibales.

```{r}
# Barplot of sex
ggplot(df_cleaned, aes(SEX, fill = factor(SEX))) +
  geom_bar() +
  labs(title = "Distribution of SEX", x = "SEX", y = "Count")

# Barplot of race
ggplot(df_cleaned, aes(RACE, fill = factor(RACE))) +
  geom_bar() +
  labs(title = "Distribution of RACE", x = "RACE", y = "Count")

# Density plot of Y
ggplot(df_cleaned, aes(Y)) +
  geom_density(fill = "steelblue", alpha = 0.5) +
  labs(title = "Density Plot of Y", x = "Y", y = "Density") +
  theme_light()

# Histogram of RACE
ggplot(df_cleaned, aes(AGE)) +
  geom_histogram(bins = 30, alpha = 0.7, color = "black", fill = "navyblue") +
  labs(title = "Distribution of AGE", x = "AGE", y = "Frequency") +
  theme_minimal()

# Histogram of HT
ggplot(df_cleaned, aes(HT)) +
  geom_histogram(bins = 30, alpha = 0.7, color = "black", fill = "navyblue") +
  labs(title = "Distribution of HT", x = "HT", y = "Frequency") +
  theme_minimal()

# Density plot of HT
ggplot(df_cleaned, aes(HT)) +
  geom_density(fill = "navyblue", alpha = 0.5) +
  labs(title = "Density of HT", x = "HT", y = "Density") +
  theme_bw()

# Histogram of WT
ggplot(df_cleaned, aes(WT)) +
  geom_histogram(bins = 30, alpha = 0.7, color = "black", fill = "navyblue") +
  labs(title = "Distribution of WT", x = "WT", y = "Frequency") +
  theme_minimal()

# Density plot of WT
ggplot(df_cleaned, aes(WT)) +
  geom_density(fill = "navyblue", alpha = 0.5) +
  labs(title = "Density of WT", x = "WT", y = "Density") + 
  theme_classic()

```

Brief discriptioln of the distribution plots:

The purpose of generating these distribution plots is to understand the characteristics of the variables in the dataset and to visually explore their relationships and patterns. Here’s a breakdown of the importance of each type of plot:

1- Distribution of SEX: The bar plot of "SEX" helps us understand the balance or imbalance between the two categories and due to the lack of a codebook, we don't know how "1" and "2" is designated to sexes. It provides insight into whether the dataset is skewed towards one category, which is important when considering potential biases or when using "SEX" as a predictor in models. In this case, the plot shows a clear imbalance, with a much higher count for category 1 than for category 2.

2- Distribution of RACE: The bar plot of "RACE" shows how the different racial categories are distributed in the dataset. It is important to identify any disparities in the representation of different groups. This can be crucial for assessing the generalizability of any results or models. The plot shows a concentration of observations in categories 1 and 2, with a much smaller presence in other categories, indicating a potential concentration in certain racial groups.

3- Density Plot of Y: The density plot of "Y" helps visualize the distribution and concentration of values for this continuous variable. This type of plot highlights the skewness or normality of the distribution, which is essential for choosing the right statistical methods. The plot reveals a skew towards lower values, which suggests that most observations have relatively lower values of "Y," with fewer higher values.

4- Distribution of AGE: The histogram of "AGE" helps us understand the spread and concentration of age across the dataset. It shows whether certain age groups are overrepresented or underrepresented, which can be important when making inferences or generalizations. The plot reveals a multimodal distribution, indicating that there are age clusters, potentially suggesting different subgroups or patterns within the data.

5- Distribution of HT: The histogram of "HT" gives us insight into the distribution of HT(potentially means height) in the dataset. Understanding this distribution can help with detecting outliers or inconsistencies in the data and provides a sense of the central tendency and variability. In this case, it shows a concentration around 1.8, indicating that most individuals in the sample have HT near this value.

6- Distribution of WT: The histogram of "WT" helps assess the spread of WT(potentially means weight) in the dataset. By visually inspecting the distribution, we can check for skewness, outliers, or any unusual patterns. The relatively normal distribution suggests that WT is evenly distributed around the peak of 80, and the spread indicates the variability in WT within the sample.

# Pair/correlation plots.

```{r}

# Select numeric variables to plot correlations
numeric_vars <- df_cleaned %>% select(Y, AGE, WT, HT)

# Compute the correlation matrix
cor_matrix <- cor(numeric_vars, use = "complete.obs")

# Plot the correlation matrix
corrplot(cor_matrix, method = "circle")


# Create a pair plot to show scatterplots and correlations
ggpairs(df_cleaned %>% select(Y, AGE, WT, HT))

```

Brief description: The pair and correlation plots provide a clear overview of the relationships between the continuous variables "Y," "AGE," "WT," and "HT." In the first plot, which is a correlation matrix, the size and color of the circles reflect the strength of the correlations between variables. The darker circles indicate stronger correlations, while lighter circles show weaker correlations. From this plot, we observe that "Y" is negatively correlated with "WT" and "HT," meaning that as weight and height increase, "Y" tends to decrease, though the correlation with "HT" is weaker than with "WT." "AGE" has weak correlations with both "Y" and "WT," and there is a moderate negative correlation between "HT" and "WT," suggesting that as height increases, weight tends to decrease slightly.

The second plot, which includes scatterplots and correlation coefficients, provides a more detailed view of these relationships. For instance, the scatterplot between "Y" and "WT" shows a slight negative relationship, with a correlation coefficient of -0.213, while the relationship between "Y" and "HT" is also slightly negative at -0.158. The scatterplot between "WT" and "HT" reveals a stronger positive correlation of 0.600, meaning that weight and height tend to increase together. These plots are useful for understanding both the visual and statistical relationships between the variables, highlighting patterns and providing specific numerical values for these associations.

#Fit models.

```{r}

# Fit the first model with DOSE as the predictor
model1 <- lm(Y ~ DOSE, data = df_cleaned)

# Fit the second model with all predictors
model2 <- lm(Y ~ ., data = df_cleaned)

# Compute RMSE and R-squared for Model 1 (DOSE only)
pred1 <- predict(model1, df_cleaned)  # Predict values
rmse1 <- sqrt(mean((pred1 - df_cleaned$Y)^2))  # RMSE calculation
r_squared1 <- summary(model1)$r.squared  # R-squared for Model 1

# Compute RMSE and R-squared for Model 2 (all predictors)
pred2 <- predict(model2, df_cleaned)  # Predict values
rmse2 <- sqrt(mean((pred2 - df_cleaned$Y)^2))  # RMSE calculation
r_squared2 <- summary(model2)$r.squared  # R-squared for Model 2

# Print the results
cat("Model 1 (DOSE only):\n")
cat("RMSE: ", rmse1, "\n")
cat("R-squared: ", r_squared1, "\n\n")

cat("Model 2 (All predictors):\n")
cat("RMSE: ", rmse2, "\n")
cat("R-squared: ", r_squared2, "\n")

```

Interpretation: The first model, using only "DOSE" as the predictor for the continuous outcome "Y," had an RMSE of 666.31, indicating that the model's predictions were fairly far from the actual values. The R-squared value of 0.515 suggests that "DOSE" alone explains about 51.5% of the variation in "Y." While this indicates a moderate relationship, it also shows that other factors are likely influencing "Y."

When we included all predictors in the second model, the RMSE decreased to 590.31, showing that the predictions were more accurate. The R-squared increased to 0.62, meaning the model now explains 62% of the variation in "Y." This improvement highlights that the additional predictors, like AGE, WT, HT, etc contribute valuable information in explaining the outcome.

In summary, while "DOSE" explains most of the variation in "Y," adding more predictors improves the model's performance and provides a more complete understanding of the outcome.

```{r}
# Step 1: Fit a logistic regression model with DOSE as the main predictor for SEX
model1 <- glm(SEX ~ DOSE, data = df_cleaned, family = binomial)

# Step 2: Fit a logistic regression model with all predictors
model2 <- glm(SEX ~ ., data = df_cleaned, family = binomial)

# Step 3: Compute predictions for both models
# For Model 1 (DOSE only), get the predicted probabilities
pred1_prob <- predict(model1, df_cleaned, type = "response")
# For Model 2 (all predictors), get the predicted probabilities
pred2_prob <- predict(model2, df_cleaned, type = "response")

# Convert probabilities to binary outcomes based on a threshold of 0.5
pred1_class <- ifelse(pred1_prob > 0.5, 1, 0)
pred2_class <- ifelse(pred2_prob > 0.5, 1, 0)

# Step 4: Compute accuracy for both models
accuracy1 <- mean(pred1_class == df_cleaned$SEX)  # Model 1 accuracy
accuracy2 <- mean(pred2_class == df_cleaned$SEX)  # Model 2 accuracy

# Step 5: Compute ROC-AUC for both models
roc1 <- roc(df_cleaned$SEX, pred1_prob)  # ROC curve for Model 1
roc2 <- roc(df_cleaned$SEX, pred2_prob)  # ROC curve for Model 2

# Step 6: Print the results
cat("Model 1 (DOSE only):\n")
cat("Accuracy: ", accuracy1, "\n")
cat("ROC-AUC: ", auc(roc1), "\n\n")

cat("Model 2 (All predictors):\n")
cat("Accuracy: ", accuracy2, "\n")
cat("ROC-AUC: ", auc(roc2), "\n")

```

Interpretation: For predicting the binary outcome "SEX," the first model with "DOSE" as the only predictor performed poorly, with an accuracy of 0 and a ROC-AUC of 0.59, indicating that "DOSE" alone does not effectively predict "SEX."

However, when we included all predictors in the second model, the accuracy improved to 0.02, and the ROC-AUC jumped to 0.98. This dramatic improvement suggests that the other predictors, like age, race, and weight, are much more relevant for predicting "SEX" than "DOSE" alone. The high ROC-AUC indicates that the model with all predictors can effectively distinguish between the two categories of "SEX."

In conclusion, while "DOSE" alone doesn’t help predict "SEX," including multiple predictors greatly enhances the model’s ability to correctly classify "SEX" and shows the importance of considering a wider range of factors.

# Module 10:

# Part 1

# Data Preparation

```{r}
# Removing RACE variable
dat_10 <- df_cleaned %>% 
  select(Y, DOSE, AGE, SEX, WT, HT)

# Checking the dimensions of the data
dim(dat_10) 

# Quick look at the data structure
glimpse(dat_10)


# Set a fixed random seed value for reproducibility
rngseed <- 1234

# Train/Test Split (75% train, 25% test)
# Set seed before splitting so the sample is reproducible.

set.seed(rngseed)
data_split <- initial_split(dat_10, prop = 0.75)
train_data <- training(data_split)
test_data  <- testing(data_split)
```

# Model fitting

```{r}
# Model Fitting using tidymodels
# We will fit:
# Model 1: Y ~ DOSE
# Model 2: Y ~ DOSE + AGE + SEX + WT + HT
# And a null model: predicting the mean of Y


# Define a linear regression specification (using lm)
lm_spec <- linear_reg() %>% 
  set_engine("lm")

# Workflow for Model 1: using only DOSE as predictor
wf_model1 <- workflow() %>%
  add_formula(Y ~ DOSE) %>%
  add_model(lm_spec)

# Workflow for Model 2: using all predictors
wf_model2 <- workflow() %>%
  add_formula(Y ~ DOSE + AGE + SEX + WT + HT) %>%
  add_model(lm_spec)

# Fit the models using only the training data
fit_model1 <- wf_model1 %>% fit(data = train_data)
fit_model2 <- wf_model2 %>% fit(data = train_data)
```

# Model performance assessment 1

```{r}

# Compute predictions on the training data for the two models

# Model 1: Only DOSE as predictor
preds_model1 <- predict(fit_model1, new_data = train_data) %>%
  bind_cols(train_data %>% select(Y))

# Model 2: All predictors (DOSE, AGE, SEX, WT, HT)
preds_model2 <- predict(fit_model2, new_data = train_data) %>%
  bind_cols(train_data %>% select(Y))


# 2. Compute RMSE for Model 1 and Model 2 using the yardstick function
rmse_model1 <- rmse(preds_model1, truth = Y, estimate = .pred)
rmse_model2 <- rmse(preds_model2, truth = Y, estimate = .pred)


# Compute RMSE of a null model (predicts the mean of Y) "by hand"
mean_Y <- mean(train_data$Y)
preds_null <- train_data %>%
  mutate(.pred = mean_Y)

rmse_null <- rmse(preds_null, truth = Y, estimate = .pred)


# Display the RMSE values for comparison
cat("RMSE for Null Model:", rmse_null$.estimate, "\n")
cat("RMSE for Model 1 (Only DOSE):", rmse_model1$.estimate, "\n")
cat("RMSE for Model 2 (All Predictors):", rmse_model2$.estimate, "\n")

```

These RMSE values show that both the dose-only model and the full model outperform the null model, which simply predicts the mean of Y. Model 1 (only DOSE) reduces the RMSE from 948.35 to about 702.79, and adding the other predictors in Model 2 brings the RMSE down further to around 627.27. In other words, each step of adding relevant predictors lowers the model’s error, suggesting that dose alone explains some variability in Y, but the additional predictors (AGE, SEX, WT, HT) offer further improvement.

# Model performance assessment 2

```{r}

# Set the random seed for CV reproducibility
set.seed(1234)


# Create 10-fold cross-validation folds from the training data
cv_folds <- vfold_cv(train_data, v = 10)


# Evaluate the models using cross-validation
# Compute RMSE for each fold and then average


# For Model 1 (using only DOSE as predictor)
cv_res_model1 <- fit_resamples(
  wf_model1,
  resamples = cv_folds,
  metrics = metric_set(rmse)
)

# For Model 2 (using all predictors: DOSE, AGE, SEX, WT, HT)
cv_res_model2 <- fit_resamples(
  wf_model2,
  resamples = cv_folds,
  metrics = metric_set(rmse)
)


# Collect and display the cross-validated RMSE metrics
cv_metrics_model1 <- collect_metrics(cv_res_model1)
cv_metrics_model2 <- collect_metrics(cv_res_model2)

cat("10-fold Cross-Validated RMSE Metrics for Model 1 (Only DOSE):\n")
print(cv_metrics_model1)

cat("\n10-fold Cross-Validated RMSE Metrics for Model 2 (All Predictors):\n")
print(cv_metrics_model2)


# Compare with the Null Model
# Since the null model simply predicts the mean, its RMSE remains unchanged.

# Compute the null model RMSE "by hand"
mean_Y <- mean(train_data$Y)
preds_null <- train_data %>% mutate(.pred = mean_Y)
rmse_null <- rmse(preds_null, truth = Y, estimate = .pred)
cat("\nRMSE for the Null Model (predicts mean of Y):", rmse_null$.estimate, "\n")

```

These 10-fold cross-validation results confirm that both the dose-only model (Model 1) and the full model (Model 2) outperform the null model (RMSE ≈ 948). Among the two, Model 2 achieves a lower average RMSE (≈ 653) than Model 1 (≈ 697), indicating that including additional predictors beyond dose reduces prediction error further. The standard errors (≈ 68 for Model 1 and 64 for Model 2) reflect the variability across the folds; even with that variability, Model 2’s mean cross-validated RMSE is still notably below Model 1’s, suggesting that it is the better-performing model overall.

# Change random seed

```{r}
# Effect of Changing the Random Seed on CV Estimates
# Create new CV folds with a different seed and compare RMSE.

set.seed(4321)  # A different random seed
cv_folds_alt <- vfold_cv(train_data, v = 10)

cv_res_model1_alt <- fit_resamples(
  wf_model1,
  resamples = cv_folds_alt,
  metrics = metric_set(rmse)
)
cv_metrics_model1_alt <- collect_metrics(cv_res_model1_alt)

cat("\n10-fold Cross-Validated RMSE Metrics for Model 1 with a different seed:\n")
print(cv_metrics_model1_alt)

```

The 10-fold cross-validated RMSE for Model 1 with a different seed is 696.3747, with a standard error of about 67.95. This value is very similar to the earlier cross-validated RMSE for Model 1 (approximately 696.71), suggesting that the model's performance is robust to changes in the random seed and that the cross-validation results are stable.

# This section added by Murphy John

## Setup

```{r}
library(dplyr)
library(ggplot2)
```

## Model Predictions

```{r}
# combine observed data and predicted data from all three models
# use preds_model1, preds_model2, and preds_null

# update column names in pred dfs
colnames(preds_model1) = c("Y_mod1", "Y_obs")
colnames(preds_model2) = c("Y_mod2", "Y_obs")
colnames(preds_null) = c("Y_obs","DOSE","AGE","SEX","WT","HT","Y_null")

# combine obs values and pred values
dat <- preds_null %>%
  full_join(preds_model1, by="Y_obs") %>%
  full_join(preds_model2, by="Y_obs") %>%
  select(
    "Y_obs",
    "Y_null",
    "Y_mod1",
    "Y_mod2"
  )
```

```{r}
# pivot data for plotting
dat1 <- dat %>%
  tidyr::pivot_longer(cols = c(Y_null, Y_mod1, Y_mod2),
                      names_to = "model", values_to = "pred")

# add model type labels
dat1$model <- factor(dat1$model, 
                     levels = c("Y_mod1", "Y_mod2", "Y_null"), 
                     labels = c("Univariable Model", "Multivariable Model", "Null Model"))

# plot observed vs predicted values
ggplot(dat1, aes(x=Y_obs, y=pred, color=model)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  scale_x_continuous(limits=c(0,5000)) +
  scale_y_continuous(limits=c(0,5000)) 

```

```{r}
# plot residuals vs predicted for multivariable model
dat$resid_mod2 <- dat$Y_mod2 - dat$Y_obs

ggplot(dat, aes(x=Y_mod2, y=resid_mod2)) +
  geom_point() +
  geom_abline(slope=0, intercept = 0) +
  scale_y_continuous(limits=c(-2500,2500))
```

## Model predictions and uncertainty

```{r}
# set seed
set.seed(rngseed)

# bootstrap 100 times
bootstrap <- rsample::bootstraps(train_data, times=100)

# fit model 2 to each sample and make predictions
pred <- purrr::map(bootstrap$splits, function(split) {
  # extract sample
  dat_boot <- rsample::analysis(split)
  # fit model
  model <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data=dat_boot)
  # make predictions
  predict(model, newdata=train_data)
})

# convert predictions to df
dat_pred <- as.data.frame(do.call(rbind, pred))

# calculate median and 89% CI
preds <- as.data.frame(
  dat_pred %>% apply(2, quantile, c(0.055, 0.5, 0.945)) |> t() 
)
colnames(preds) <- list("lo", "median", "high")

dat_plot <- cbind(dat, preds)
```

```{r}
# plot predictions and bootstrap medians and CIs
ggplot(dat_plot) +
  geom_point(aes(x = Y_obs, y = median, color = "Bootstrap Median"), 
             size=2) +
  geom_errorbar(aes(x = Y_obs, y = median, ymin = lo, ymax = high,
                color="Bootstrap Median")) +
  geom_point(aes(x = Y_obs, y = Y_mod2, color = "Original Model"), 
             size=2) +
  geom_abline(slope = 1, intercept = 0) +
  scale_x_continuous(limits=c(0,5700)) +
  scale_y_continuous(limits=c(0,5700)) + 
  scale_color_manual(values = c(
    "Bootstrap Median" = "slateblue", 
    "Original Model" = "gray30"
    )) +
  labs(
    x = "Observed Values",
    y = "Predicted Values"
  )
```

The fitted results from the bootstrapped samples are very similar to those of model 2. Most, if not all, of the 89% confidence intervals computed from the bootstrapped samples contain the fitted estimate of model 2. The data points follow the x = y line very closely, which means that our predicted values are similar to our observed values; i.e., model 2 is performing well.

## End of part 2

------------------------------------------------------------------------

# Part 3: Back to Murtaza Yaqubi

# Final Evaluation Using Test Data

```{r}
# Make predictions on the test data using the full model (Model 2)
# (fit_model2 was obtained using the training data in Part 1)
preds_all_test <- predict(fit_model2, test_data) %>% 
  bind_cols(test_data)

# Compute RMSE for test data using Model 2
rmse_all_test <- rmse(preds_all_test, truth = Y, estimate = .pred)

# Prepare an evaluation list with RMSE metrics and best model designation.
# Here, rmse_model1 is the RMSE for the dose-only model, rmse_model2 is for the full model on training data, and rmse_null is the RMSE for the null model.
evaluation_list <- list(
  RMSE_DOSE    = rmse_model1$.estimate,
  RMSE_ALL     = rmse_model2$.estimate,
  RMSE_ALL_TEST= rmse_all_test$.estimate,
  Best_Model   = "All Predictors",
  RMSE_NULL    = rmse_null$.estimate
)
print(evaluation_list)
```

Interpretaion: The evaluation metrics show that the null model has an RMSE of 948.3526, the dose-only model has an RMSE of 702.7909, and the full model ("All Predictors") achieves an RMSE of 627.2724 on the training data and 518.2239 on the test data, indicating that the full model outperforms the others.

# Combine training and test predictions for plotting.

```{r}
# For training predictions, use preds_model2 from Part 1.
preds_all_train <- preds_model2 %>% 
  rename(.pred = Y_mod2, Y = Y_obs) %>% 
  mutate(Data = "Train")

# For test predictions, use fit_model2 from Part 1.
preds_all_test <- predict(fit_model2, test_data) %>% 
  bind_cols(test_data) %>% 
  mutate(Data = "Test")

# Combine training and test prediciotns 
combined_preds <- bind_rows(preds_all_train, preds_all_test)

# Plot predicted vs. observed values for both training and test data
ggplot(combined_preds, aes(x = Y, y = .pred, color = Data)) +
  geom_point(alpha = 0.6) +
  scale_x_continuous(limits = c(0, 6000)) +  # Adjust x-axis limits as needed
  scale_y_continuous(limits = c(0, 6000)) +  # Adjust y-axis limits as needed
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Predicted vs. Observed Values (Model 2)",
    x = "Observed Y",
    y = "Predicted Y",
    color = "Dataset"
  ) +
  theme_minimal()

```

Conclusion: This plot shows predicted versus observed values for both the training (blue points) and test (pink points) sets under the full model (Model 2). Most points cluster close to the diagonal (the dashed line), indicating that the model predicts Y accurately for both datasets. The primary “confusion” it reveals is that the test data actually fits in about the same range as the training data—there is no clear sign of higher error or systematic bias in the test set. Often one expects the test predictions to be slightly worse if the model were overfitted, but here the test points blend well with the training data points. This can happen if the test set has fewer extreme observations or if both sets come from the same distribution and the model generalizes effectively, which might initially surprise someone expecting a visible difference in the test results.
