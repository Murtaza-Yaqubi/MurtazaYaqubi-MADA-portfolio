---
title: "ml-model-exercise"
author: "Murtaza Yaqubi"
editor: visual
---

## Let's load the packages we need for this analysis

```{r}
library(here)
library(corrplot)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(readr)
```

## Set random seed for reproducibility

```{r}
# Setting seed for reproducibility
rdmseed <- 1234
set.seed(rdmseed)
```

## Loading dataset

```{r}
# load data 
ml_df <- read_csv(here::here("ml-models-exercise/data/df_cleaned.csv"))

# check the columns of the data
head(ml_df)
```

## Data Processing

```{r}
# Convert categorical variables to numeric so they're usable in models
ml_df$SEX <- as.numeric(ml_df$SEX)
ml_df$RACE <- as.numeric(ml_df$RACE)

# combine categories 7 and 88 of the race variable
ml_df$race_clean <- case_when(
  ml_df$RACE %in% c(7, 88) ~ 3,
  TRUE ~ as.numeric(ml_df$RACE)
)
```

## EDA - Correlation

```{r}
# Generate correlation matrix to understand relationships between numeric predictors
cor_matrix <- cor(ml_df %>% select(Y, AGE, WT, HT))

# Make the correlation plot
corrplot::corrplot(
  cor_matrix, 
  method = 'color',         # Use colored heatmap instead of plain numbers
  col = colorRampPalette(c("darkblue", "gray", "firebrick"))(200),
  addCoef.col = "black",    # Add correlation coefficients in black text
  tl.col = "black",          # Axis labels in black
  tl.cex = 1.1,              # Slightly larger axis text
  number.cex = 0.9,          # Font size for correlation values
  cl.cex = 0.9,              # Color legend text size
  cl.lim = c(-1, 1),         # Color scale limits
  diag = FALSE              # Remove diagonal since it's always 1
)
```

## Feature engineering

```{r}
# Creating a new variable: Body Mass Index (BMI)
ml_df$BMI <- ml_df$WT / (ml_df$HT)^2

# Checking a summary of the BMI values
summary(ml_df$BMI)

#Histogram to visualize distribution of BMI
ggplot(ml_df, aes(x = BMI)) +
  geom_histogram(fill = "orange", color = "black", bins = 30, alpha = 0.7) +
  labs(
    title = "Histogram of BMI",
    x = "BMI",
    y = "Count"
  ) +
  theme_minimal()
```

## Model building

```{r}
# Reset seed before modeling
set.seed(rdmseed)
```

## Linear model

```{r}
# Fit a standard linear regression model using all predictors
lm_model <- linear_reg() %>%
  fit(Y ~ DOSE + AGE + SEX + race_clean + BMI, data = ml_df)

# Get predictions from linear model and calculate RMSE
ml_df$pred_lm <- predict(lm_model, new_data = ml_df)$.pred
rmse(ml_df, truth = Y, estimate = pred_lm)
```

```{r}
# # Plot observed vs. predicted for Linear Model
ggplot(ml_df, aes(x = Y, y = pred_lm)) +
  geom_point(color = "darkblue", alpha = 0.7, size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", size = 1) +
  scale_x_continuous(limits = c(0, 6000)) +
  scale_y_continuous(limits = c(0, 4000))
```

## LASSO

```{r}
set.seed(rdmseed)

# Prepare recipe for LASSO with normalization
rec <- recipe(Y ~ DOSE + AGE + SEX + race_clean + BMI, data = ml_df) %>%
  step_normalize(all_numeric(), -all_outcomes())

# Define LASSO model with a fixed penalty
lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet")

# Create a workflow and fit the LASSO model
wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lasso_spec)

lasso_fit <- fit(wf, data = ml_df)
ml_df$pred_lasso <- predict(lasso_fit, new_data = ml_df)$.pred
rmse(ml_df, truth = Y, estimate = pred_lasso)
```

```{r}
# Plot observed vs. predicted for LASSO Model
ggplot(ml_df, aes(x = Y, y = pred_lasso)) +
  geom_point(color = "darkviolet", alpha = 0.7, size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", size = 1) +
  scale_x_continuous(limits = c(0, 6000)) +
  scale_y_continuous(limits = c(0, 4000))
```

## Random Forest

```{r}
set.seed(rdmseed)

# Create a workflow and fit the LASSO model
rf_model <- rand_forest() %>%
  set_mode("regression") %>%
  set_engine("ranger", seed = rdmseed)

rf_fit <- rf_model %>%
  fit(Y ~ DOSE + AGE + SEX + race_clean + BMI, data = ml_df)

# Get predictions and evaluate model performance
ml_df$pred_rf <- predict(rf_fit, ml_df)$.pred
rmse(ml_df, truth = Y, estimate = pred_rf)
```

```{r}
# Plot observed vs. predicted for RF Model
ggplot(ml_df, aes(x = Y, y = pred_rf)) +
  geom_point(color = "darkgreen", alpha = 0.7, size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", size = 1) +
  scale_x_continuous(limits = c(0, 6000)) +
  scale_y_continuous(limits = c(0, 4000))
```

## Tuning the LASSO model (without cross-validation)

```{r}
set.seed(rdmseed)

# Create a grid of penalty values for LASSO
lasso_grid <- tibble(penalty = 10^seq(-5, 2, length.out = 50))

# Define LASSO model with tunable penalty
lasso_spec_tune <- linear_reg(penalty = tune()) %>%
  set_engine("glmnet")

# Create workflow and assign to lasso_tuning_workflow (new name)
lasso_tuning_workflow <- workflow() %>%
  add_model(lasso_spec_tune) %>%
  add_recipe(rec)

# (Optional) Diagnostic check
print("Objects in the environment:")
print(ls())
if (!"lasso_tuning_workflow" %in% ls()) {
  stop("Error: lasso_tuning_workflow was not created!")
}

# Perform grid search without CV (apparent resampling)
lasso_tune <- tune_grid(
  lasso_tuning_workflow,
  resamples = apparent(ml_df),
  grid = lasso_grid,
  metrics = metric_set(rmse)
)

# Extract tuning results
lasso_tune_df <- as.data.frame(lasso_tune$.metrics)

# Plot LASSO tuning results
ggplot(lasso_tune_df, aes(x = penalty, y = .estimate)) +
  geom_line(size = 1, color = "steelblue") +
  geom_point(size = 2, color = "firebrick") +
  scale_x_log10() +
  labs(
    x = "Penalty (log scale)",
    y = "RMSE",
    title = "LASSO Tuning (No Cross-validation)",
    subtitle = "Using apparent resampling"
  ) +
  theme_minimal()

```

## Tuning for our Random Forest model (without cross-validation)

```{r}
# Create tuning grid for mtry and min_n
rf_grid <- grid_regular(
  mtry(range = c(1, 7)),
  min_n(range = c(1, 21)),
  levels = 7
)

# Define random forest model with tunable parameters
rf_spec_tune <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 300
) %>%
  set_mode("regression") %>%
  set_engine("ranger", seed = rdmseed)

# Build workflow
rf_wf_tune <- workflow() %>%
  add_model(rf_spec_tune) %>%
  add_recipe(rec)

# Perform RF tuning without CV
rf_tune <- tune_grid(
  rf_wf_tune,
  resamples = apparent(ml_df),
  grid = rf_grid,
  metrics = metric_set(rmse)
)

rf_tune_df <- as.data.frame(rf_tune$.metrics)

# Plot RF tuning results
ggplot(rf_tune_df, aes(x = mtry, y = min_n, fill = .estimate)) +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(
    title = "Random Forest Tuning (No Cross-validation)",
    x = "mtry",
    y = "min_n",
    fill = "RMSE"
  ) +
  theme_minimal()
```

## Cross-validation for our LASSO model tuning

```{r}
set.seed(rdmseed)

# Create 5-fold CV with 5 repeats
cv_folds <- vfold_cv(ml_df, v = 5, repeats = 5)

# (Optional) Check that the workflow exists before tuning
if (!exists("lasso_tuning_workflow")) {
  stop("Error: lasso_tuning_workflow is missing! Ensure the tuning chunk ran correctly.")
} else {
  print("lasso_tuning_workflow exists. Proceeding with CV tuning.")
}

# Tune LASSO using CV
lasso_tune_cv <- tune_grid(
  lasso_tuning_workflow,
  resamples = cv_folds,
  grid = lasso_grid,
  metrics = metric_set(rmse)
)

autoplot(lasso_tune_cv)


```

## Cross-validation tuning for our random forest model

```{r}
# Tune RF using CV
rf_tune_cv <- tune_grid(
  rf_wf_tune,
  resamples = cv_folds,
  grid = rf_grid,
  metrics = metric_set(rmse)
)
autoplot(rf_tune_cv)
```
